#!/usr/bin/env python3

import tensorflow as tf
import sys
import os
import time
import numpy as np
from models.embedding_network import EmbeddingNetwork
import argparse
import glob
from datetime import datetime
import shutil


def find_tfrecord_for(data_type, search_root):
    return glob.glob('{}/{}*.tfrecord'.format(search_root, data_type))


def ask_to_clean_dir(dir_path):
    if len(os.listdir(dir_path)) != 0:
        choice = input('Do you want to delete all the files in the {}? (y/n)'.format(dir_path)).lower()
        if choice == 'y' or choice == 'yes':
            shutil.rmtree(dir_path)
            os.mkdir(dir_path)
            return True
        else:
            print('{} is not empty, it is impossible to update the data inside this folder.'.format(dir_path))
            return False
    return True


def build_graph(neighbors, attributes, u_init, labels, attr_dim, emb_size, T, relu_layer_num, max_node_num, classifier_layer_nums):
    embedding_network = EmbeddingNetwork(relu_layer_num, max_node_num, emb_size, attr_dim, T, True)
    program_emb = embedding_network.embed(neighbors, attributes, u_init)

    layer_logits = [program_emb]
    with tf.variable_scope("detector_nn"):
        for idx, layer_num in enumerate(classifier_layer_nums):
            if idx != len(classifier_layer_nums) - 1:
                layer_logits.append(tf.contrib.layers.fully_connected(layer_logits[-1], num_outputs=layer_num, scope='layer_{}'.format(idx)))
            else:
                layer_logits.append(tf.contrib.layers.fully_connected(layer_logits[-1], num_outputs=layer_num, scope='layer_{}'.format(idx), activation_fn=None))
        pred_classes = tf.argmax(layer_logits[-1], axis=1)
        loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(labels, 2), logits=layer_logits[-1]))
    acc_op = tf.contrib.metrics.accuracy(labels=labels, predictions=pred_classes)
    return loss_op, acc_op, pred_classes, embedding_network, 


def parse_example_function(example_proto):
    features = {
                "labels":            tf.FixedLenFeature((), dtype=tf.int64),
                "neighbors_shape":  tf.FixedLenFeature((2), dtype=tf.int64),
                "attributes_shape": tf.FixedLenFeature((2), dtype=tf.int64),
                "u_init_shape":    tf.FixedLenFeature((2), dtype=tf.int64),
                "identifiers":       tf.FixedLenFeature((), dtype=tf.string),
                "neighbors":        tf.VarLenFeature(dtype=tf.float32),
                "attributes":       tf.VarLenFeature(dtype=tf.float32),
                "u_init":          tf.VarLenFeature(dtype=tf.float32),
               }
    parsed_features = tf.parse_single_example(example_proto, features)
    for feature_name in parsed_features:
        if feature_name in ['labels', 'neighbors_shape', 'attributes_shape', 'u_init_shape', 'identifiers']:
            continue
        parsed_features[feature_name] = tf.sparse_tensor_to_dense(parsed_features[feature_name])
        parsed_features[feature_name] = tf.reshape(parsed_features[feature_name], parsed_features[feature_name + '_shape'])
    return parsed_features["neighbors"], parsed_features["attributes"], parsed_features["u_init"], parsed_features["labels"], parsed_features["identifiers"]


def main(argv):
    parser = argparse.ArgumentParser(description='Train the malware classification model with graph embedding.')
    parser.add_argument('TRAIN_DATA_DIR', help='The path to the directory contains training data.')
    parser.add_argument('MODEL_DIR', help='The folder to save the model.')
    parser.add_argument('LOG_DIR', help='The folder to save the model log.')
    parser.add_argument('--EMB_MODEL_DIR', help='The folder to load the program embedding model.')
    parser.add_argument('--LoadModel', dest='LoadModel', help='Load old model in MODEL_DIR.', action='store_true')
    parser.add_argument('--no-LoadModel', dest='LoadModel', help='Do not load old model in MODEL_DIR.', action='store_false')
    parser.set_defaults(LoadModel=False)
    parser.add_argument('--UpdateModel', dest='UpdateModel', help='Update the model.', action='store_true')
    parser.add_argument('--no-UpdateModel', dest='UpdateModel', help='Do not update the model.', action='store_false')
    parser.set_defaults(UpdateModel=False)
    parser.add_argument('--GPU_ID', type=int, default=0, help='The GPU ID of the GPU card.')
    parser.add_argument('--BatchSize', type=int, default=32, help='Number of step per-epoch.')
    parser.add_argument('--LearningRate', type=float, default=0.0001, help='The learning rate for the model.')
    parser.add_argument('--T', type=int, default=5, help='The T parameter in the model.(How many hops to propagate information to.)')
    parser.add_argument('--AttrDims', type=int, default=8, help='The dimensions of the attributes.')
    parser.add_argument('--MaxNodeNum', type=int, default=200, help='The max number of nodes per ACFG.')
    parser.add_argument('--Epochs', type=int, default=1000, help='The number of epochs to run.')
    parser.add_argument('--NumberOfRelu', type=int, default=2, help='The number of relu layer in the sigma function.')
    parser.add_argument('--EmbeddingSize', type=int, default=64, help='The dimension of the embedding vectors.')
    parser.add_argument('--MaxNumModelToKeep', type=int, default=100, help='The number of model to keep in the saver directory.')
    parser.add_argument('--TF_LOG_LEVEL', default=3, type=int, help='Environment variable to TF_CPP_MIN_LOG_LEVEL')
    args = parser.parse_args()

    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"   # see issue #152
    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.GPU_ID)
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(args.TF_LOG_LEVEL)


    if not os.path.isdir(args.MODEL_DIR):
        print('MODEL_DIR folder should be a valid folder.')
        sys.exit(-2)
    elif not args.LoadModel:
        if not ask_to_clean_dir(args.MODEL_DIR):
            sys.exit(-3)
        if not ask_to_clean_dir(args.LOG_DIR):
            sys.exit(-4)


    with tf.device('/cpu:0'):
        shuffle_seed = tf.placeholder(tf.int64, shape=[])
        train_filenames = find_tfrecord_for('train', args.TRAIN_DATA_DIR)
        dataset = tf.data.TFRecordDataset(train_filenames)
        dataset = dataset.map(parse_example_function, num_parallel_calls=8)
        dataset = dataset.shuffle(buffer_size=10000, seed=shuffle_seed).batch(args.BatchSize)
        dataset = dataset.prefetch(buffer_size=4000)
        iterator = dataset.make_initializable_iterator()
        next_element = iterator.get_next()
        
        test_filenames = find_tfrecord_for('test', args.TRAIN_DATA_DIR)
        test_dataset = tf.data.TFRecordDataset(test_filenames)
        test_dataset = test_dataset.map(parse_example_function, num_parallel_calls=8)
        test_iterator = test_dataset.make_one_shot_iterator()
        test_next_element = test_iterator.get_next()

    print('Building model graph...... [{}]'.format(str(datetime.now())))
    with tf.device('/cpu:0'):
        global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)

    
    neighbors = tf.placeholder(tf.float32, shape=(None, args.MaxNodeNum, args.MaxNodeNum), name='neighbors')
    attributes = tf.placeholder(tf.float32, shape=(None, args.MaxNodeNum, args.AttrDims), name='attributes')
    u_inits = tf.placeholder(tf.float32, shape=(None, args.MaxNodeNum, args.EmbeddingSize), name='u_inits')
    labels = tf.placeholder(tf.int64, shape=(None,), name='labels')
    loss_op, acc_op, pred_class, embedding_network = build_graph(neighbors, attributes, u_inits, labels, args.AttrDims, args.EmbeddingSize, args.T, args.NumberOfRelu, args.MaxNodeNum, [40, 20, 2])

    with tf.name_scope('Accuracy'):
        tf.summary.scalar('accuracy', acc_op)
    with tf.name_scope('Cost'):
        tf.summary.scalar('loss', loss_op)
    merged = tf.summary.merge_all()

    train_op = tf.train.AdamOptimizer(args.LearningRate).minimize(loss_op, global_step=global_step)

    print('Starting the tensorflow session...... [{}]'.format(str(datetime.now())))
    with tf.Session() as sess:
        train_writer = tf.summary.FileWriter(os.path.join(args.LOG_DIR, 'train'), sess.graph)
        if not args.LoadModel and args.EMB_MODEL_DIR:
            emb_model_saver = tf.train.Saver({'siamese/W1': embedding_network.W1, 'siamese/W2': embedding_network.W2, 'siamese/P_n_0': embedding_network.P_n[0], 'siamese/P_n_1': embedding_network.P_n[1]}, max_to_keep=args.MaxNumModelToKeep)
        saver = tf.train.Saver(max_to_keep=args.MaxNumModelToKeep)

        all_vars= tf.global_variables()
        def get_var(name):
            for i in range(len(all_vars)):
                if all_vars[i].name.startswith(name):
                    return all_vars[i]
            return None

        if args.LoadModel:
            print('Loading the stored model...... [{}]'.format(str(datetime.now())))
            states = tf.train.get_checkpoint_state(args.MODEL_DIR)
            saver.restore(sess, states.model_checkpoint_path)
            sess.run(tf.local_variables_initializer())
        elif args.EMB_MODEL_DIR:
            init_op = tf.global_variables_initializer()
            sess.run(init_op)
            sess.run(tf.local_variables_initializer())
            states = tf.train.get_checkpoint_state(args.EMB_MODEL_DIR)
            emb_model_saver.restore(sess, states.model_checkpoint_path)
        else:
            init_op = tf.global_variables_initializer()
            sess.run(init_op)
            sess.run(tf.local_variables_initializer())

        print('Start in training mode. [{}]'.format(str(datetime.now())))
        epoch_loss = float('Inf')
        total_step = int(sess.run(global_step))
        train_acc = 0
        test_acc = 0

        test_neighbors = []
        test_attributes = []
        test_u_inits = []
        test_labels = []
        print('\tLoading testing dataset.... [{}]'.format(str(datetime.now())))
        while True:
            try:
                test_neighbor, test_attribute, test_u_init, test_label, identifier = sess.run(test_next_element)
            except tf.errors.OutOfRangeError:
                break
            test_neighbors.append(test_neighbor)
            test_attributes.append(test_attribute)
            test_u_inits.append(test_u_init)
            test_labels.append(test_label)

        cur_epoch = 0

        sess.run(iterator.initializer, feed_dict={shuffle_seed: cur_epoch})
        print('\tStart training epoch...... [{}]'.format(str(datetime.now())))
        while True:
            try:
                # Training Phase
                cur_neighbors, cur_attributes, cur_u_inits, cur_labels, identifiers = sess.run(next_element)

                if args.UpdateModel:
                    sess.run(train_op, {
                        neighbors: cur_neighbors, attributes: cur_attributes, u_inits: cur_u_inits,
                        labels: cur_labels
                    })

                loss, batch_acc = sess.run([loss_op, acc_op], {
                    neighbors: cur_neighbors, attributes: cur_attributes, u_inits: cur_u_inits,
                    labels: cur_labels
                })

                sys.stdout.write('Epoch: {:6}, BatchLoss: {:8.7f}, TotalStep: {:7}, TrainAcc: {:.4f}, TestAcc: {:.4f}  \r'.format(cur_epoch, loss, total_step, batch_acc, test_acc))
                sys.stdout.flush()

                if args.UpdateModel:
                    summary = sess.run(merged, {
                        neighbors: cur_neighbors, attributes: cur_attributes, u_inits: cur_u_inits,
                        labels: cur_labels
                    })
                    train_writer.add_summary(summary, total_step)

                    if os.path.isabs(args.MODEL_DIR):
                        relative_model_dir = os.path.relpath(args.MODEL_DIR, os.getcwd())
                    saver.save(sess, os.path.join(relative_model_dir, 'model.ckpt'), global_step=global_step)
                total_step = int(sess.run(global_step))

            except tf.errors.OutOfRangeError:
                # Testing Phase
                test_acc = sess.run(acc_op, {
                    neighbors:  test_neighbors, attributes: test_attributes, u_inits: test_u_inits,
                    labels: test_labels
                })
                if args.UpdateModel:
                    test_acc_summary = tf.Summary()
                    test_acc_summary.value.add(tag='Accuracy/test_accuracy', simple_value=test_acc)
                    train_writer.add_summary(test_acc_summary, total_step)
                print('Epoch: {:6}, BatchLoss: {:8.7f}, TotalStep: {:7}, TrainAcc: {:.4f}, TestAcc: {:.4f}'.format(cur_epoch, loss, total_step, batch_acc, test_acc))
                sys.stdout.flush()
                cur_epoch += 1

                if cur_epoch < args.Epochs:
                    sess.run(iterator.initializer, feed_dict={shuffle_seed: cur_epoch})
                else:
                    print('Training finished. [{}]'.format(str(datetime.now())))
                    break



if __name__ == '__main__':
    main(sys.argv)
