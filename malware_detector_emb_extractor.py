#!/usr/bin/env python3

import tensorflow as tf
import sys
import os
import time
import numpy as np
from models.embedding_network import EmbeddingNetwork
import argparse
import glob
from datetime import datetime
import pickle
import csv


def find_tfrecord_for(data_type, search_root):
    return glob.glob('{}/{}*.tfrecord'.format(search_root, data_type))


def parse_example_function(example_proto):
    features = {
                "labels":            tf.FixedLenFeature((), dtype=tf.int64),
                "neighbors_shape":  tf.FixedLenFeature((2), dtype=tf.int64),
                "attributes_shape": tf.FixedLenFeature((2), dtype=tf.int64),
                "u_init_shape":    tf.FixedLenFeature((2), dtype=tf.int64),
                "identifiers":       tf.FixedLenFeature((), dtype=tf.string),
                "neighbors":        tf.VarLenFeature(dtype=tf.float32),
                "attributes":       tf.VarLenFeature(dtype=tf.float32),
                "u_init":          tf.VarLenFeature(dtype=tf.float32),
               }
    parsed_features = tf.parse_single_example(example_proto, features)
    for feature_name in parsed_features:
        if feature_name in ['labels', 'neighbors_shape', 'attributes_shape', 'u_init_shape', 'identifiers']:
            continue
        parsed_features[feature_name] = tf.sparse_tensor_to_dense(parsed_features[feature_name])
        parsed_features[feature_name] = tf.reshape(parsed_features[feature_name], parsed_features[feature_name + '_shape'])
    return parsed_features["neighbors"], parsed_features["attributes"], parsed_features["u_init"], parsed_features["labels"], parsed_features["identifiers"]


def main(argv):
    parser = argparse.ArgumentParser(description='Train the malware classification model with graph embedding.')
    parser.add_argument('TRAIN_DATA_DIR', help='The path to the directory contains training data.')
    parser.add_argument('MODEL_DIR', help='The folder to save the model.')
    parser.add_argument('TSNE_DIR', help='The path of the TSNE mode folder.')
    parser.add_argument('--GPU_ID', type=int, default=0, help='The GPU ID of the GPU card.')
    parser.add_argument('--BatchSize', type=int, default=32, help='Number of step per-epoch.')
    parser.add_argument('--T', type=int, default=5, help='The T parameter in the model.(How many hops to propagate information to.)')
    parser.add_argument('--AttrDims', type=int, default=8, help='The dimensions of the attributes.')
    parser.add_argument('--MaxNodeNum', type=int, default=200, help='The max number of nodes per ACFG.')
    parser.add_argument('--NumberOfRelu', type=int, default=2, help='The number of relu layer in the sigma function.')
    parser.add_argument('--EmbeddingSize', type=int, default=64, help='The dimension of the embedding vectors.')
    parser.add_argument('--TF_LOG_LEVEL', default=3, type=int, help='Environment variable to TF_CPP_MIN_LOG_LEVEL')
    args = parser.parse_args()

    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"   # see issue #152
    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.GPU_ID)
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(args.TF_LOG_LEVEL)


    if not os.path.isdir(args.MODEL_DIR):
        print('MODEL_DIR folder should be a valid folder.')
        sys.exit(-2)

    with tf.device('/cpu:0'):
        shuffle_seed = tf.placeholder(tf.int64, shape=[])
        train_filenames = find_tfrecord_for('train', args.TRAIN_DATA_DIR)
        dataset = tf.data.TFRecordDataset(train_filenames)
        dataset = dataset.map(parse_example_function, num_parallel_calls=8)
        dataset = dataset.shuffle(buffer_size=10000, seed=shuffle_seed).batch(args.BatchSize)
        dataset = dataset.prefetch(buffer_size=4000)
        iterator = dataset.make_initializable_iterator()
        next_element = iterator.get_next()
        
        test_filenames = find_tfrecord_for('test', args.TRAIN_DATA_DIR)
        test_dataset = tf.data.TFRecordDataset(test_filenames)
        test_dataset = test_dataset.map(parse_example_function, num_parallel_calls=8)
        test_dataset = test_dataset.shuffle(buffer_size=10000, seed=shuffle_seed).batch(args.BatchSize)
        test_iterator = test_dataset.make_initializable_iterator()
        test_next_element = test_iterator.get_next()

    print('Building model graph...... [{}]'.format(str(datetime.now())))
    neighbors = tf.placeholder(tf.float32, shape=(None, args.MaxNodeNum, args.MaxNodeNum), name='neighbors')
    attributes = tf.placeholder(tf.float32, shape=(None, args.MaxNodeNum, args.AttrDims), name='attributes')
    u_inits = tf.placeholder(tf.float32, shape=(None, args.MaxNodeNum, args.EmbeddingSize), name='u_inits')

    embedding_network = EmbeddingNetwork(args.NumberOfRelu, args.MaxNodeNum, args.EmbeddingSize, args.AttrDims, args.T, False)
    program_emb = embedding_network.embed(neighbors, attributes, u_inits)

    print('Starting the tensorflow session...... [{}]'.format(str(datetime.now())))
    with tf.Session() as sess:
        emb_model_saver = tf.train.Saver({'W1': embedding_network.W1, 'W2': embedding_network.W2, 'P_n_0': embedding_network.P_n[0], 'P_n_1': embedding_network.P_n[1]})

        print('Loading the stored model...... [{}]'.format(str(datetime.now())))
        states = tf.train.get_checkpoint_state(args.MODEL_DIR)
        emb_model_saver.restore(sess, states.model_checkpoint_path)

        emb_ids = []
        output_embs = []
        labels = []
        cur_epoch = 0
        sess.run(iterator.initializer, feed_dict={shuffle_seed: cur_epoch})
        print('\tStart to generate embedding vectors for data...... [{}]'.format(str(datetime.now())))
        while True:
            try:
                # Training Phase
                cur_neighbors, cur_attributes, cur_u_inits, cur_labels, identifiers = sess.run(next_element)
                ids = [x.decode('utf-8') for x in identifiers]

                embs = sess.run(program_emb, {
                    neighbors: cur_neighbors, attributes: cur_attributes, u_inits: cur_u_inits,
                })
                if output_embs == []:
                    emb_ids = ids
                    output_embs = embs.tolist()
                    labels = cur_labels.tolist()
                else:
                    emb_ids += ids
                    output_embs += embs.tolist()
                    labels += cur_labels.tolist()
            except tf.errors.OutOfRangeError:
                print('Generation finished. [{}]'.format(str(datetime.now())))
                break

        print('Starting the tensorflow session...... [{}]'.format(str(datetime.now())))
        with tf.Session() as sess:
            emb_plk_path = os.path.join(args.TSNE_DIR, 'embeddings.plk')
            print('Writing embeddings.plk file to {}...... [{}]'.format(emb_plk_path, str(datetime.now())))
            with open(emb_plk_path, 'wb') as f_out:
                pickle.dump({'samples': output_embs}, f_out)
            metadata_path = os.path.join(args.TSNE_DIR, 'metadata.tsv')
            print('Writing metadata.csv file to {}...... [{}]'.format(metadata_path, str(datetime.now())))
            with open(metadata_path, 'w', newline='') as csvfile:
                csv_writer = csv.writer(csvfile, delimiter='\t', quotechar='\'', quoting=csv.QUOTE_MINIMAL)
                csv_writer.writerow(['dim{}'.format(x) for x in range(args.EmbeddingSize)] + ['label'] + ['color'])
                for idx, emb in enumerate(output_embs):
                    row = emb
                    row.append(emb_ids[idx])
                    if labels[idx] == 1:
                        row.append('red')
                    else:
                        row.append('green')
                    csv_writer.writerow(row)
            print('Generate embedding vectors successfully. To view the visualization, please run:\n$ python3 -m evaluation_scripts.utils.create_tsne_projector {} {} YOUR_EMBEDDING_LOG_DIR'.format(emb_plk_path, metadata_path))

if __name__ == '__main__':
    main(sys.argv)
