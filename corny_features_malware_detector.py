#!/usr/bin/env python3

from tensorflow.python.client import device_lib
import pickle
import glob
import numpy as np
import tensorflow as tf
import sys
import argparse
import os
import re
from datetime import datetime
import shutil


def get_available_gpus():
    local_device_protos = device_lib.list_local_devices()
    return [x.name for x in local_device_protos if x.device_type == 'GPU']

def ask_to_clean_dir(dir_path):
    if len(os.listdir(dir_path)) != 0:
        choice = input('Do you want to delete all the files in the {}? (y/n)'.format(dir_path)).lower()
        if choice == 'y' or choice == 'yes':
            shutil.rmtree(dir_path)
            os.mkdir(dir_path)
            return True
        else:
            print('{} is not empty, it is impossible to update the data inside this folder.'.format(dir_path))
            return False
    return True


def find_tfrecord_for(data_type, search_root):
    return glob.glob('{}/{}*.tfrecord'.format(search_root, data_type))


def parse_example_function(example_proto):
    features = {
                "labels":         tf.FixedLenFeature((), dtype=tf.int64),
                "features_shape": tf.FixedLenFeature((1), dtype=tf.int64),
                "identifiers":    tf.FixedLenFeature((), dtype=tf.string),
                "features":       tf.VarLenFeature(dtype=tf.float32),
               }
    parsed_features = tf.parse_single_example(example_proto, features)
    for feature_name in parsed_features:
        if feature_name in ['labels', 'features_shape', 'identifiers']:
            continue
        parsed_features[feature_name] = tf.sparse_tensor_to_dense(parsed_features[feature_name])
        parsed_features[feature_name] = tf.reshape(parsed_features[feature_name], parsed_features[feature_name + '_shape'])
    return parsed_features["features"], parsed_features["labels"], parsed_features["identifiers"]


def main(argv):
    parser = argparse.ArgumentParser(description='Detect malware using some old school features.')
    parser.add_argument('TRAIN_DATA_DIR', help='The path to the training data directory.')
    parser.add_argument('MODEL_DIR', help='The path to the directory to save the trained model.')
    parser.add_argument('LOG_DIR', help='The folder to save the model log.')
    parser.add_argument('--LoadModel', dest='LoadModel', help='Load old model in MODEL_DIR.', action='store_true')
    parser.add_argument('--no-LoadModel', dest='LoadModel', help='Do not load old model in MODEL_DIR.', action='store_false')
    parser.set_defaults(LoadModel=False)
    parser.add_argument('--UpdateModel', dest='UpdateModel', help='Update the model.', action='store_true')
    parser.add_argument('--no-UpdateModel', dest='UpdateModel', help='Do not update the model.', action='store_false')
    parser.set_defaults(UpdateModel=False)
    parser.add_argument('--FeatureDim', type=int, default=1486, help='The dimension of the input features.')
    parser.add_argument('--GPU_ID', type=int, default=0, help='The GPU ID of the GPU card.')
    parser.add_argument('--BatchSize', type=int, default=32, help='Number of step per-epoch.')
    parser.add_argument('--LearningRate', type=float, default=0.0001, help='The learning rate for the model.')
    parser.add_argument('--Epochs', type=int, default=1000, help='The number of epochs to run.')
    parser.add_argument('--MaxNumModelToKeep', type=int, default=100, help='The number of model to keep in the saver directory.')
    parser.add_argument('--ShuffleLearningData', dest='ShuffleLearningData', help='Learning data shuffle mode on', action='store_true')
    parser.add_argument('--no-ShuffleLearningData', dest='ShuffleLearningData', help='Learning data shuffle mode off', action='store_false')
    parser.set_defaults(ShuffleLearningData=False)
    parser.add_argument('--TF_LOG_LEVEL', default=3, type=int, help='Environment variable to TF_CPP_MIN_LOG_LEVEL')
    args = parser.parse_args()

    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"   # see issue #152
    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.GPU_ID)
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(args.TF_LOG_LEVEL)

    if not os.path.isdir(args.MODEL_DIR):
        print('MODEL_DIR folder should be a valid folder.')
        sys.exit(-2)
    elif not args.LoadModel:
        if not ask_to_clean_dir(args.MODEL_DIR):
            sys.exit(-3)
        if not ask_to_clean_dir(args.LOG_DIR):
            sys.exit(-4)

    # Prepare for the input datasets.
    with tf.device('/cpu:0'):
        shuffle_seed = tf.placeholder(tf.int64, shape=[])
        train_filenames = find_tfrecord_for('train', args.TRAIN_DATA_DIR)
        dataset = tf.data.TFRecordDataset(train_filenames)
        dataset = dataset.map(parse_example_function, num_parallel_calls=8)
        dataset = dataset.shuffle(buffer_size=10000, seed=shuffle_seed).batch(args.BatchSize)
        dataset = dataset.prefetch(buffer_size=4000)
        iterator = dataset.make_initializable_iterator()
        next_element = iterator.get_next()

        test_filenames = find_tfrecord_for('test', args.TRAIN_DATA_DIR)
        test_dataset = tf.data.TFRecordDataset(test_filenames)
        test_dataset = test_dataset.map(parse_example_function, num_parallel_calls=8)
        test_iterator = test_dataset.make_one_shot_iterator()
        test_next_element = test_iterator.get_next()


    # Declare placeholder.
    with tf.device('/cpu:0'):
        global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)
        features = tf.placeholder(tf.float32, shape=(None, args.FeatureDim), name='features')
        labels = tf.placeholder(tf.int64, shape=(None,), name='labels')

    # Build the graph.
    with tf.device('/device:GPU:0'):
        nn_layer_nums = [100, 50, 20, 2]
        layer_logits = [features]
        with tf.variable_scope("detector_nn"):
            for idx, layer_num in enumerate(nn_layer_nums):
                if idx != len(nn_layer_nums) - 1:
                    layer_logits.append(tf.contrib.layers.fully_connected(layer_logits[-1], num_outputs=layer_num, scope='layer_{}'.format(idx)))
                else:
                    layer_logits.append(tf.contrib.layers.fully_connected(layer_logits[-1], num_outputs=layer_num, scope='layer_{}'.format(idx), activation_fn=None))
            pred_classes = tf.argmax(layer_logits[-1], axis=1)
            loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(labels, 2), logits=layer_logits[-1]))
        acc_op = tf.contrib.metrics.accuracy(labels=labels, predictions=pred_classes)
        train_op = tf.train.AdamOptimizer(args.LearningRate).minimize(loss_op, global_step=global_step)
    with tf.name_scope('Accuracy'):
        tf.summary.scalar('accuracy', acc_op)
    with tf.name_scope('Cost'):
        tf.summary.scalar('loss', loss_op)
    merged = tf.summary.merge_all()

    # Start session.
    print('Starting the tensorflow session...... [{}]'.format(str(datetime.now())))
    with tf.Session() as sess:
        train_writer = tf.summary.FileWriter(os.path.join(args.LOG_DIR, 'train'), sess.graph)
        # Try to load the model if it exist.
        saver = tf.train.Saver(max_to_keep=args.MaxNumModelToKeep)


        all_vars= tf.global_variables()
        def get_var(name):
            for i in range(len(all_vars)):
                if all_vars[i].name.startswith(name):
                    return all_vars[i]
            return None

        w0 = get_var('detector_nn/layer_0/weights')
        w1 = get_var('detector_nn/layer_1/weights')
        w2 = get_var('detector_nn/layer_2/weights')

        if args.LoadModel:
            print('Loading the stored model...... [{}]'.format(str(datetime.now())))
            states = tf.train.get_checkpoint_state(args.MODEL_DIR)
            saver.restore(sess, states.model_checkpoint_path)
            sess.run(tf.local_variables_initializer())
        else:
            init_op = tf.global_variables_initializer()
            sess.run(init_op)
            sess.run(tf.local_variables_initializer())

        # Read all the test samples
        test_features = []
        test_labels = []
        print('\tLoading testing dataset.... [{}]'.format(str(datetime.now())))
        while True:
            try:
                test_feature, test_label, identifiers = sess.run(test_next_element)
            except tf.errors.OutOfRangeError:
                break
            test_features.append(test_feature)
            test_labels.append(test_label)

        # Start the training phase.
        cur_epoch = 0
        total_step = int(sess.run(global_step))
        sess.run(iterator.initializer, feed_dict={shuffle_seed: cur_epoch})
        print('\tStart training epoch...... [{}]'.format(str(datetime.now())))
        while True:
            try:
                # Training Phase
                cur_features, cur_labels, identifiers = sess.run(next_element)

                # if args.UpdateModel:
                sess.run(train_op, {
                    features: cur_features,
                    labels: cur_labels
                })

                loss, batch_acc = sess.run([loss_op, acc_op], {
                    features: cur_features,
                    labels: cur_labels
                })

                # Write the summary to the disk.
                if args.UpdateModel:
                    summary = sess.run(merged, {
                        features: cur_features,
                        labels: cur_labels
                    })
                    train_writer.add_summary(summary, total_step)

                    # Save the model to the disk.
                    if os.path.isabs(args.MODEL_DIR):
                        relative_model_dir = os.path.relpath(args.MODEL_DIR, os.getcwd())
                    saver.save(sess, os.path.join(relative_model_dir, 'model.ckpt'), global_step=global_step)
                total_step = int(sess.run(global_step))
            except tf.errors.OutOfRangeError:
                # Testing Phase
                test_acc = sess.run(acc_op, {
                    features: test_features,
                    labels: test_labels
                })
                if args.UpdateModel:
                    test_acc_summary = tf.Summary()
                    test_acc_summary.value.add(tag='Accuracy/test_accuracy', simple_value=test_acc)
                    train_writer.add_summary(test_acc_summary, total_step)
                print('Epoch: {:6}, BatchLoss: {:8.7f}, TotalStep: {:7}, TrainAcc: {:.4f}, TestAcc: {:.4f}'.format(cur_epoch, loss, total_step, batch_acc, test_acc))
                sys.stdout.flush()
                cur_epoch += 1
                if cur_epoch < args.Epochs:
                    # Seeding the datasets with new round seed.
                    sess.run(iterator.initializer, feed_dict={shuffle_seed: cur_epoch})
                else:
                    print('Training finished. [{}]'.format(str(datetime.now())))
                    break

if __name__ == '__main__':
    main(sys.argv)
